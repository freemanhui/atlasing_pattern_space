{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APS Experimental Results Analysis\n",
    "\n",
    "This notebook provides interactive analysis of all experimental results from Phases 5.2-5.5.\n",
    "\n",
    "**Contents:**\n",
    "1. Load and explore all results\n",
    "2. Ablation study visualization\n",
    "3. OOD robustness analysis\n",
    "4. Few-shot learning evaluation\n",
    "5. Component contribution analysis\n",
    "6. Statistical significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ablation study results\n",
    "ablation_path = Path(\"../outputs/ablation/ablation_summary.json\")\n",
    "with open(ablation_path, \"r\") as f:\n",
    "    ablation_results = json.load(f)\n",
    "\n",
    "print(\"Loaded ablation results for configurations:\")\n",
    "print(list(ablation_results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OOD results\n",
    "ood_dir = Path(\"../outputs/ood/metrics\")\n",
    "ood_results = {}\n",
    "for json_file in ood_dir.glob(\"*_ood_results.json\"):\n",
    "    config_name = json_file.stem.replace(\"_ood_results\", \"\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        ood_results[config_name] = json.load(f)\n",
    "\n",
    "print(f\"Loaded OOD results for {len(ood_results)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load few-shot results\n",
    "fewshot_dir = Path(\"../outputs/fewshot/metrics\")\n",
    "fewshot_results = {}\n",
    "for json_file in fewshot_dir.glob(\"*_fewshot_results.json\"):\n",
    "    config_name = json_file.stem.replace(\"_fewshot_results\", \"\")\n",
    "    with open(json_file, \"r\") as f:\n",
    "        fewshot_results[config_name] = json.load(f)\n",
    "\n",
    "print(f\"Loaded few-shot results for {len(fewshot_results)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ablation Study Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "configs = [\"baseline\", \"t_only\", \"c_only\", \"e_only\", \"t_c\", \"t_e\", \"c_e\", \"t_c_e\"]\n",
    "metrics_data = []\n",
    "\n",
    "for config in configs:\n",
    "    if config in ablation_results:\n",
    "        metrics = ablation_results[config][\"metrics\"]\n",
    "        metrics_data.append({\n",
    "            \"Config\": config,\n",
    "            \"Reconstruction\": metrics[\"reconstruction_error\"],\n",
    "            \"Trustworthiness\": metrics[\"trustworthiness\"],\n",
    "            \"Continuity\": metrics[\"continuity\"],\n",
    "            \"kNN Preservation\": metrics[\"knn_preservation\"],\n",
    "            \"ARI\": metrics[\"ari\"],\n",
    "            \"NMI\": metrics[\"nmi\"],\n",
    "            \"Silhouette\": metrics[\"silhouette\"],\n",
    "        })\n",
    "\n",
    "df_ablation = pd.DataFrame(metrics_data)\n",
    "df_ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topology metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = [\"Trustworthiness\", \"Continuity\", \"kNN Preservation\"]\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    df_ablation.plot(x=\"Config\", y=metric, kind=\"bar\", ax=ax, legend=False)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"Configuration\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticklabels(df_ablation[\"Config\"], rotation=45)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = [\"ARI\", \"NMI\", \"Silhouette\"]\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    df_ablation.plot(x=\"Config\", y=metric, kind=\"bar\", ax=ax, legend=False, color=\"coral\")\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"Configuration\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_xticklabels(df_ablation[\"Config\"], rotation=45)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Component Contribution Analysis\n",
    "\n",
    "Analyze how each component (T, C, E) contributes to overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvements over baseline\n",
    "baseline_metrics = ablation_results[\"baseline\"][\"metrics\"]\n",
    "\n",
    "improvements = []\n",
    "for config in [\"t_only\", \"c_only\", \"e_only\", \"t_c\", \"t_e\", \"c_e\", \"t_c_e\"]:\n",
    "    if config in ablation_results:\n",
    "        metrics = ablation_results[config][\"metrics\"]\n",
    "        improvements.append({\n",
    "            \"Config\": config,\n",
    "            \"Trust Δ%\": ((metrics[\"trustworthiness\"] - baseline_metrics[\"trustworthiness\"]) / baseline_metrics[\"trustworthiness\"]) * 100,\n",
    "            \"Cont Δ%\": ((metrics[\"continuity\"] - baseline_metrics[\"continuity\"]) / baseline_metrics[\"continuity\"]) * 100,\n",
    "            \"ARI Δ%\": ((metrics[\"ari\"] - baseline_metrics[\"ari\"]) / baseline_metrics[\"ari\"]) * 100,\n",
    "        })\n",
    "\n",
    "df_improvements = pd.DataFrame(improvements)\n",
    "df_improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of improvements\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    df_improvements.set_index(\"Config\"),\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    center=0,\n",
    "    cbar_kws={'label': 'Improvement (%)'}\n",
    ")\n",
    "plt.title(\"Improvement over Baseline (%)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OOD Robustness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs t_c_e on rotation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Rotation robustness\n",
    "ax = axes[0]\n",
    "for config in [\"baseline\", \"t_c_e\"]:\n",
    "    if config in ood_results:\n",
    "        angles = [15, 30, 45, 60]\n",
    "        accs = [ood_results[config][\"rotation\"][f\"rot_{a}\"][\"knn_accuracy\"] for a in angles]\n",
    "        ax.plot(angles, accs, marker='o', linewidth=2, label=config)\n",
    "\n",
    "ax.set_xlabel(\"Rotation Angle (degrees)\")\n",
    "ax.set_ylabel(\"kNN Accuracy\")\n",
    "ax.set_title(\"Rotation Robustness\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Noise robustness\n",
    "ax = axes[1]\n",
    "for config in [\"baseline\", \"t_c_e\"]:\n",
    "    if config in ood_results:\n",
    "        noise_levels = [0.1, 0.2, 0.3, 0.5]\n",
    "        accs = [ood_results[config][\"noise\"][f\"noise_{s}\"][\"knn_accuracy\"] for s in noise_levels]\n",
    "        ax.plot(noise_levels, accs, marker='o', linewidth=2, label=config)\n",
    "\n",
    "ax.set_xlabel(\"Noise σ\")\n",
    "ax.set_ylabel(\"kNN Accuracy\")\n",
    "ax.set_title(\"Noise Robustness\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Few-Shot Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare few-shot performance across configurations\n",
    "k_shots = [1, 3, 5, 10]\n",
    "configs_to_compare = [\"baseline\", \"t_only\", \"e_only\", \"t_c_e\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "methods = [\"proto\", \"logreg\", \"knn\"]\n",
    "method_names = [\"Prototypical\", \"Logistic Regression\", \"k-NN\"]\n",
    "\n",
    "for ax, method, name in zip(axes, methods, method_names):\n",
    "    for config in configs_to_compare:\n",
    "        if config in fewshot_results:\n",
    "            means = [fewshot_results[config][f\"{k}_shot\"][f\"{method}_mean\"] for k in k_shots]\n",
    "            stds = [fewshot_results[config][f\"{k}_shot\"][f\"{method}_std\"] for k in k_shots]\n",
    "            ax.errorbar(k_shots, means, yerr=stds, marker='o', linewidth=2, capsize=5, label=config)\n",
    "    \n",
    "    ax.set_xlabel(\"k (shots per class)\")\n",
    "    ax.set_ylabel(\"Test Accuracy\")\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if \"baseline\" in ablation_results and \"t_c_e\" in ablation_results:\n",
    "    baseline = ablation_results[\"baseline\"][\"metrics\"]\n",
    "    full_aps = ablation_results[\"t_c_e\"][\"metrics\"]\n",
    "    \n",
    "    print(\"\\nAblation Study (t_c_e vs baseline):\")\n",
    "    for metric in [\"trustworthiness\", \"continuity\", \"ari\", \"nmi\", \"silhouette\"]:\n",
    "        improvement = ((full_aps[metric] - baseline[metric]) / baseline[metric]) * 100\n",
    "        print(f\"  {metric:20s}: {baseline[metric]:.4f} → {full_aps[metric]:.4f} ({improvement:+.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Findings\n",
    "\n",
    "### Topology Preservation (T)\n",
    "- **Effect**: Improves trustworthiness and continuity metrics\n",
    "- **Best for**: Maintaining local neighborhood structure\n",
    "\n",
    "### Causality/Independence (C)\n",
    "- **Effect**: Reduces spurious correlations\n",
    "- **Best for**: OOD robustness\n",
    "\n",
    "### Energy Basins (E)\n",
    "- **Effect**: Creates clearer cluster structure\n",
    "- **Best for**: Few-shot learning and clustering\n",
    "\n",
    "### Full APS (T+C+E)\n",
    "- **Synergy**: Components work together complementarily\n",
    "- **Best overall**: Achieves highest scores across most metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
