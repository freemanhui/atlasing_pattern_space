# APS Experiments - Progress Summary

**Date**: January 2025  
**Branch**: `005-experiments`

## Overview

This document tracks the completion of the experimental infrastructure for the Atlasing Pattern Space (APS) framework. All core experimental scripts, utilities, and documentation are now production-ready.

---

## âœ… Completed Phases

### Phase 5.1: Infrastructure & Metrics âœ“

**Objective**: Build foundational utilities for data loading and model evaluation.

**Completed Components**:

1. **Dataset Utilities** (`experiments/utils/datasets.py`)
   - `get_mnist_loaders()`: Standard MNIST train/test with normalization
   - `get_rotated_mnist()`: Rotated MNIST for OOD testing
   - `get_noisy_mnist()`: Gaussian noise injection at multiple Ïƒ levels
   - `get_fashion_mnist()`: FashionMNIST for transfer evaluation
   - `sample_k_shot()`: k-shot sampling for few-shot learning

2. **Metrics Module** (`experiments/utils/metrics.py`)
   - **Topology**: `trustworthiness()`, `continuity()`, `knn_preservation()`
   - **Clustering**: `clustering_metrics()` (ARI, NMI, silhouette)
   - **Independence**: `hsic_independence()` (RBF kernel HSIC)
   - **Task**: `reconstruction_error()`, `few_shot_accuracy()`
   - **Unified**: `evaluate_model()` - comprehensive metric aggregation

3. **Module Exports** (`experiments/utils/__init__.py`)
   - Clean API for importing datasets and metrics

**Validation**: All utilities tested with shape checks and basic functionality tests.

---

### Phase 5.2: Baseline MNIST Experiment âœ“

**Objective**: Validate training and evaluation pipeline with a baseline experiment.

**Script**: `experiments/mnist_baseline.py`

**Features**:
- Train/eval modes
- 4 configurations: baseline, t-only, e-only, t+e
- Comprehensive metrics: reconstruction, topology, clustering
- 2D embedding visualization with scatter plots
- Auto-saves: checkpoints (`.pt`), metrics (`.json`), plots (`.png`)
- Device auto-detection (CPU/CUDA/MPS)

**Quick Test**:
```bash
python experiments/mnist_baseline.py --config baseline --epochs 5
```

**Validation**: Successfully trained baseline model in 5 epochs, generated metrics and plots. Pipeline confirmed working.

**Output Structure**:
```
outputs/baseline/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ {config}_model.pt
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ {config}_metrics.json
â””â”€â”€ plots/
    â””â”€â”€ {config}_embedding.png
```

---

### Phase 5.3: Full Ablation Study âœ“

**Objective**: Systematically evaluate all component combinations.

**Script**: `experiments/mnist_ablation.py`

**Configurations** (8 total):

| Config    | Topology (T) | Causality (C) | Energy (E) |
|-----------|--------------|---------------|------------|
| baseline  | âœ—            | âœ—             | âœ—          |
| t_only    | âœ“            | âœ—             | âœ—          |
| c_only    | âœ—            | âœ“             | âœ—          |
| e_only    | âœ—            | âœ—             | âœ“          |
| t_c       | âœ“            | âœ“             | âœ—          |
| t_e       | âœ“            | âœ—             | âœ“          |
| c_e       | âœ—            | âœ“             | âœ“          |
| t_c_e     | âœ“            | âœ“             | âœ“          |

**Evaluation Metrics**:
- Reconstruction error (MSE)
- Topology preservation (trustworthiness, continuity, kNN preservation)
- Clustering quality (ARI, NMI, silhouette)
- Independence (HSIC between latent and labels)

**Features**:
- Unified `AblationModel` with selective component activation
- Auto-saves: checkpoints, per-config metrics, embeddings, plots
- Summary table comparing all configurations
- Ready for full 50-epoch runs

**Usage**:
```bash
python experiments/mnist_ablation.py --epochs 50 --device mps
```

**Output Structure**:
```
outputs/ablation/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ baseline.pt
â”‚   â”œâ”€â”€ t_only.pt
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ t_c_e.pt
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ baseline_metrics.json
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ t_c_e_metrics.json
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ baseline_embedding.png
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ t_c_e_embedding.png
â””â”€â”€ ablation_summary.json
```

---

### Phase 5.4: OOD Robustness âœ“

**Objective**: Test generalization under distribution shifts.

**Script**: `experiments/mnist_ood.py`

**OOD Scenarios**:

1. **Rotated MNIST**: 15Â°, 30Â°, 45Â°, 60Â°
2. **Noisy MNIST**: Gaussian noise Ïƒ = 0.1, 0.2, 0.3, 0.5
3. **FashionMNIST**: Zero-shot transfer evaluation

**Evaluation**:
- Reconstruction error (robustness to corruption)
- Topology preservation (structural degradation)
- Clustering quality (class separability)
- kNN accuracy (latent space classification)

**Features**:
- Loads trained checkpoints from ablation study
- Visualizes all OOD embeddings in grid layout
- Summary table showing degradation across scenarios
- Compares robustness across model configurations

**Usage**:
```bash
# Evaluate t_c_e configuration on all OOD scenarios
python experiments/mnist_ood.py --config t_c_e --device mps
```

**Output Structure**:
```
outputs/ood/
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ {config}_ood_results.json
â””â”€â”€ plots/
    â””â”€â”€ {config}_ood_embeddings.png  # Grid: Original + 4 rotations + 4 noise + Fashion
```

---

### Phase 5.5: Few-Shot Learning âœ“

**Objective**: Test if learned embeddings support efficient few-shot learning.

**Script**: `experiments/mnist_fewshot.py`

**k-shot Settings**: 1, 3, 5, 10 examples per class

**Classifiers**:
1. **Logistic Regression**: Linear decision boundary in latent space
2. **k-Nearest Neighbors**: Non-parametric local classification
3. **Prototypical Network**: Distance to class centroids

**Evaluation**:
- Accuracy averaged over multiple random trials (default: 5 trials)
- Standard deviation to measure robustness
- Confusion matrices for error analysis
- Learning curves showing accuracy vs. k-shot

**Hypothesis**: Topology-preserving and energy-shaped embeddings should enable better few-shot generalization through more structured latent spaces.

**Features**:
- Averages over multiple random k-shot samples
- Generates confusion matrices and learning curves
- Summary tables with mean Â± std accuracy
- Compares all three classifier types

**Usage**:
```bash
# Evaluate t_c_e configuration with default k-shots
python experiments/mnist_fewshot.py --config t_c_e --device mps

# Custom k-shot values
python experiments/mnist_fewshot.py --config baseline --k-shots 1 5 10 20 --n-trials 10
```

**Output Structure**:
```
outputs/fewshot/
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ {config}_fewshot_results.json
â””â”€â”€ plots/
    â”œâ”€â”€ {config}_confusion_5shot.png
    â””â”€â”€ {config}_learning_curves.png  # 3-panel: LogReg | kNN | Proto
```

---

## ğŸ“Š Experimental Pipeline

### Complete Workflow

```bash
# 1. Train all configurations (Phase 5.3)
python experiments/mnist_ablation.py --epochs 50 --device mps

# 2. Evaluate OOD robustness (Phase 5.4)
for config in baseline t_only c_only e_only t_c t_e c_e t_c_e; do
    python experiments/mnist_ood.py --config $config --device mps
done

# 3. Evaluate few-shot learning (Phase 5.5)
for config in baseline t_only c_only e_only t_c t_e c_e t_c_e; do
    python experiments/mnist_fewshot.py --config $config --device mps
done
```

### Quick Single-Config Test

```bash
# Train baseline for 5 epochs (quick validation)
python experiments/mnist_baseline.py --config baseline --epochs 5

# Train t_c_e for full 50 epochs
python experiments/mnist_ablation.py --epochs 50 --device mps

# Then evaluate t_c_e on all downstream tasks
python experiments/mnist_ood.py --config t_c_e --device mps
python experiments/mnist_fewshot.py --config t_c_e --device mps
```

---

## ğŸ“‚ Repository Structure

```
experiments/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py           # Module exports
â”‚   â”œâ”€â”€ datasets.py           # Data loading utilities
â”‚   â””â”€â”€ metrics.py            # Evaluation metrics
â”œâ”€â”€ mnist_baseline.py         # Phase 5.2: Baseline validation
â”œâ”€â”€ mnist_ablation.py         # Phase 5.3: Full ablation study
â”œâ”€â”€ mnist_ood.py              # Phase 5.4: OOD robustness
â”œâ”€â”€ mnist_fewshot.py          # Phase 5.5: Few-shot learning
â”œâ”€â”€ README.md                 # User-facing documentation
â””â”€â”€ PROGRESS_SUMMARY.md       # This file

outputs/
â”œâ”€â”€ baseline/                 # Phase 5.2 outputs
â”œâ”€â”€ ablation/                 # Phase 5.3 outputs
â”œâ”€â”€ ood/                      # Phase 5.4 outputs
â””â”€â”€ fewshot/                  # Phase 5.5 outputs
```

---

## ğŸ¯ Research Questions Addressed

### 1. Component Effectiveness (Phase 5.3)
- **Q**: How does each component (T, C, E) contribute to embedding quality?
- **A**: Systematic ablation quantifies individual and combined effects on reconstruction, topology, clustering, and independence.

### 2. Robustness (Phase 5.4)
- **Q**: Do TCE embeddings maintain structure under distribution shifts?
- **A**: OOD experiments measure degradation across rotations, noise, and domain transfer.

### 3. Few-Shot Learning (Phase 5.5)
- **Q**: Does better latent structure enable more efficient learning?
- **A**: k-shot experiments test if topology/energy shaping improves sample efficiency.

### 4. Synergy vs. Redundancy
- **Q**: Are components complementary or redundant?
- **A**: Pairwise configurations (t_c, t_e, c_e) reveal interaction effects.

---

## ğŸ§ª Next Steps

### Immediate Actions (Ready to Run)

1. **Full Ablation Run** (Phase 5.3)
   ```bash
   python experiments/mnist_ablation.py --epochs 50 --device mps
   ```
   - Expected runtime: ~2-3 hours for 8 configs Ã— 50 epochs
   - Generates all checkpoints needed for downstream tasks

2. **OOD Robustness Sweep** (Phase 5.4)
   ```bash
   for config in baseline t_c_e; do
       python experiments/mnist_ood.py --config $config --device mps
   done
   ```
   - Start with baseline and t_c_e for comparison

3. **Few-Shot Analysis** (Phase 5.5)
   ```bash
   for config in baseline t_c_e; do
       python experiments/mnist_fewshot.py --config $config --device mps
   done
   ```
   - Compare few-shot performance across configs

### Future Phases (Planned)

#### Phase 5.6: Publication Materials
- [ ] Generate publication-quality figures
- [ ] Create comparison tables across all experiments
- [ ] Statistical significance tests (paired t-tests, Wilcoxon)
- [ ] LaTeX tables for paper
- [ ] High-resolution plots with consistent styling

#### Phase 5.7: Extended Experiments
- [ ] Test on CIFAR-10 (convolutional encoder)
- [ ] Test on text data (GLOVE embeddings)
- [ ] Higher latent dimensions (16, 32, 64)
- [ ] Hyperparameter sensitivity analysis
- [ ] Runtime/memory profiling

#### Phase 5.8: Theoretical Analysis
- [ ] Topology: persistent homology validation
- [ ] Causality: independence test power analysis
- [ ] Energy: basin of attraction quantification
- [ ] Combined: emergent properties of TCE interaction

---

## ğŸ“ˆ Validation Status

| Phase | Script                  | Status | Tested | Notes                          |
|-------|-------------------------|--------|--------|--------------------------------|
| 5.1   | `utils/datasets.py`     | âœ…     | âœ…     | All loaders functional         |
| 5.1   | `utils/metrics.py`      | âœ…     | âœ…     | All metrics validated          |
| 5.2   | `mnist_baseline.py`     | âœ…     | âœ…     | 5-epoch test successful        |
| 5.3   | `mnist_ablation.py`     | âœ…     | â³     | Ready for full 50-epoch run    |
| 5.4   | `mnist_ood.py`          | âœ…     | â³     | Awaiting Phase 5.3 checkpoints |
| 5.5   | `mnist_fewshot.py`      | âœ…     | â³     | Awaiting Phase 5.3 checkpoints |

**Legend**:
- âœ… Complete
- â³ Ready but awaiting dependencies
- âŒ Not started

---

## ğŸ”¬ Key Design Decisions

### 1. Unified Model Architecture
All experiments use the same `AblationModel` class with selective component activation. This ensures:
- Fair comparison (no architecture confounds)
- Code reusability
- Consistent hyperparameters

### 2. Separate Scripts for Each Task
Rather than one monolithic script, we have:
- **mnist_baseline.py**: Quick validation
- **mnist_ablation.py**: Systematic training
- **mnist_ood.py**: Robustness evaluation
- **mnist_fewshot.py**: Few-shot evaluation

**Benefits**:
- Modular and maintainable
- Can run experiments independently
- Easy to parallelize on compute cluster

### 3. JSON Metrics + Plots
Each experiment saves:
- **JSON files**: Structured metrics for programmatic analysis
- **PNG plots**: Visual inspection and publication figures

This dual format supports both automated analysis pipelines and manual review.

### 4. Checkpoint Reuse
Phases 5.4 and 5.5 load checkpoints from Phase 5.3, avoiding redundant training.

### 5. Reproducibility
- Fixed random seeds for dataset splits
- Multiple trials for few-shot (averages over randomness)
- Saved configs alongside results

---

## ğŸš€ Running Full Experiment Suite

### Prerequisites
```bash
# Ensure environment is active
source .venv/bin/activate  # or conda activate aps

# Verify installation
python -c "import aps; print('APS installed')"
```

### Full Pipeline (~3-4 hours on M1/M2 Mac)

```bash
#!/bin/bash
# run_all_experiments.sh

set -e  # Exit on error

echo "=== Phase 5.3: Training all configurations ==="
python experiments/mnist_ablation.py --epochs 50 --device mps

echo ""
echo "=== Phase 5.4: OOD robustness ==="
for config in baseline t_only c_only e_only t_c t_e c_e t_c_e; do
    echo "Testing $config on OOD scenarios..."
    python experiments/mnist_ood.py --config $config --device mps
done

echo ""
echo "=== Phase 5.5: Few-shot learning ==="
for config in baseline t_only c_only e_only t_c t_e c_e t_c_e; do
    echo "Testing $config on few-shot learning..."
    python experiments/mnist_fewshot.py --config $config --device mps
done

echo ""
echo "=== All experiments complete! ==="
echo "Results saved in outputs/ directory"
```

Make executable and run:
```bash
chmod +x run_all_experiments.sh
./run_all_experiments.sh
```

---

## ğŸ“ Documentation Status

- [x] Code-level docstrings in all modules
- [x] User-facing README.md
- [x] Progress tracking (this file)
- [x] Quick start guide
- [x] Example usage commands
- [ ] Jupyter notebook for result analysis (Future)
- [ ] Paper figures generation script (Future)

---

## ğŸ“ Citation

If you use this experimental infrastructure, please cite:

```bibtex
@software{aps_experiments_2025,
  title = {Atlasing Pattern Space: Experimental Infrastructure},
  author = {Freeman Hui},
  year = {2025},
  url = {https://github.com/yourusername/atlasing_pattern_space}
}
```

---

## ğŸ“ Contact & Contribution

For questions or contributions:
1. Open an issue on GitHub
2. Submit a pull request with proposed changes
3. Contact: freeman.hui@example.com

---

**Last Updated**: January 2025  
**Branch**: `005-experiments`  
**Status**: âœ… Infrastructure Complete, Ready for Full Experiments
