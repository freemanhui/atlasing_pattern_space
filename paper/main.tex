\documentclass{article}

% ICML 2025 style
\usepackage[accepted]{icml2025}

% Packages
\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

% Custom commands
\newcommand{\loss}{\mathcal{L}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}

% Title and authors
\icmltitlerunning{When Does Causal Regularization Help?}

\begin{document}

\twocolumn[
\icmltitle{When Does Causal Regularization Help? \\
A Systematic Study of Boundary Conditions in Spurious Correlation Learning}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Anonymous Author(s)}{equal,yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Anonymous Institution}

\icmlcorrespondingauthor{Anonymous}{anonymous@example.com}

\icmlkeywords{Causal Learning, Spurious Correlations, Domain Generalization, Autoencoder Architectures, Empirical Analysis}

\vskip 0.3in
]

\printAffiliationsAndNotice{}

\begin{abstract}
Causal regularization methods (e.g., IRM, HSIC) promise to learn invariant predictors that ignore spurious correlations. However, their effectiveness depends critically on dataset characteristics and architectural choices that are poorly understood. We present a systematic study of \textbf{boundary conditions} for causal learning through controlled experiments on ColoredMNIST, where we precisely manipulate spurious correlation strength across four difficulty levels (90\%, 99.5\%, 100\%, and anti-correlated).

Our findings challenge common assumptions: (1) Standard autoencoder baselines are surprisingly robust, achieving 82-97\% accuracy even with 99\% spurious correlation due to \textbf{implicit causal bias} from reconstruction objectives; (2) A sharp \textbf{phase transition at 100\%} correlation reveals the necessity of causal signal---all methods fail completely; (3) Explicit causal regularization (topology, HSIC independence, energy) provides only \textbf{marginal benefits} (0-4pp) across the difficulty spectrum; (4) Memory-based energy methods exhibit severe hyperparameter sensitivity, requiring careful tuning.

Rather than claim method superiority, we provide honest analysis establishing \textbf{when causal regularization justifies its complexity}, offering practical guidelines and reproducible methodology for evaluating future causal learning approaches.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Machine learning models often exploit \textit{spurious correlations}---statistical associations between features and labels that hold in training data but fail to generalize \citep{arjovsky2019irm}. Classic examples include models that associate ``cows'' with ``grass'' backgrounds rather than cow features \citep{beery2018recognition}, or that rely on image texture rather than shape \citep{geirhos2018imagenettrained}. Such failures motivate \textit{causal learning} methods that aim to identify invariant predictors robust to distribution shifts.

However, a critical question remains underexplored: \textbf{When does explicit causal regularization actually help?} While numerous methods claim improvements \citep{arjovsky2019irm, krueger2021outofdistribution, ganin2016domain}, understanding the boundary conditions---the dataset characteristics and architectural choices that determine method effectiveness---remains limited. This gap hinders practitioners seeking to decide when causal methods justify their added complexity.

\subsection{Our Approach: Systematic Boundary Analysis}

We conduct a controlled study on ColoredMNIST \citep{arjovsky2019irm}, systematically varying spurious correlation strength to map out:
\begin{itemize}
    \item \textbf{Success regimes}: Where causal methods provide clear benefit
    \item \textbf{Failure modes}: Where all methods struggle equally
    \item \textbf{Baseline boundaries}: When simpler approaches suffice
\end{itemize}

Our experimental design spans four difficulty levels---90\% correlation (easy), 99.5\% (hard), 100\% (impossible), and anti-correlated test (very hard)---enabling precise characterization of method behavior across the correlation spectrum.

We evaluate the Atlasing Pattern Space (APS) framework as a representative causal learning approach combining three regularization strategies: topology preservation (kNN structure), HSIC independence (causal-spurious separation), and energy-based attractors (latent space shaping). APS's modular design enables rigorous ablation studies, and we explicitly \textbf{do not claim APS superiority}---rather, we use it as a case study for understanding when and why causal regularization helps in general.

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Autoencoder baselines exhibit implicit causal bias}: Standard reconstruction + classification achieves 82-97\% accuracy even with 99\% spurious correlation, suggesting that pixel-level reconstruction forces learning of structural (causal) features beyond color.
    
    \item \textbf{Phase transition at 100\% correlation}: A sharp boundary where all methods fail completely (1-5\% accuracy), validating that some causal signal is necessary---HSIC independence alone cannot ``discover'' causal features without positive examples.
    
    \item \textbf{Marginal benefits from explicit causality}: Across all difficulty levels where learning is possible, APS components provide 0-4pp improvements, often statistically insignificant, suggesting baseline architecture choices dominate.
    
    \item \textbf{Energy instability reveals hyperparameter sensitivity}: Memory-based energy methods with log-sum-exp formulations exhibit catastrophic divergence (-43M loss) without careful tuning (β, λ scaling), limiting practical applicability.
\end{enumerate}

\subsection{Contributions}

\begin{itemize}
    \item \textbf{Empirical characterization} of correlation strength vs. method effectiveness across systematic spectrum
    \item \textbf{Discovery and validation} of implicit causal bias in autoencoder architectures
    \item \textbf{Phase transition analysis} establishing necessity of causal signal
    \item \textbf{Practical guidelines} for when causal methods justify complexity
    \item \textbf{Honest negative results} advancing understanding of method limitations
    \item \textbf{Reproducible framework} for evaluating causal learning approaches
\end{itemize}

Rather than cherry-pick favorable results, we provide comprehensive analysis including failure modes and marginal cases, establishing methodology for honest evaluation of causal learning methods.

\section{Background and Related Work}
\label{sec:background}

\input{sections/02_background}

\section{The APS Framework}
\label{sec:methods}

\input{sections/03_methods}

\section{Experimental Design}
\label{sec:experiments}

\input{sections/04_experiments}

\section{Results: The Correlation Spectrum}
\label{sec:results}

\input{sections/05_results}

\section{Analysis: Why is Baseline So Strong?}
\label{sec:analysis}

\input{sections/06_analysis}

\section{Practical Guidelines}
\label{sec:guidelines}

\input{sections/07_guidelines}

\section{Limitations and Future Work}
\label{sec:limitations}

\input{sections/08_limitations}

\section{Conclusion}
\label{sec:conclusion}

\input{sections/09_conclusion}

\bibliography{references}
\bibliographystyle{icml2025}

\end{document}
